{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBTI_auswertung.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezmnJ_MEco7S"
      },
      "source": [
        "import itertools\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ2HamM-AsMP"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0b6YMmrdDha"
      },
      "source": [
        "# Importing dataset is stored locally\n",
        "df=pd.read_csv('/content/drive/MyDrive/Colab/mbti_1.csv', dtype={'type': str, 'posts': str})\n",
        "df.info()\n",
        "# Für spätere visualisierung\n",
        "mbti = {'I':'Introversion', 'E':'Extroversion', 'N':'Intuition',\n",
        "        'S':'Sensing', 'T':'Thinking', 'F': 'Feeling',\n",
        "        'J':'Judging', 'P': 'Perceiving'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXsq-hHxdFzP"
      },
      "source": [
        "#Anzahl der verschiedenen Typen zählen\n",
        "df_counted = df['type'].value_counts()\n",
        "df_counted.head()\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.barplot(x = df_counted.index, y = df_counted.values)\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Types', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7qdfn8yAjYF"
      },
      "source": [
        "# Vorverarbeitung\n",
        "- ' Am anfang und Ende des Strings entfernen\n",
        "- Nach ||| in eine Liste an Posts Splitten\n",
        "- lowercase\n",
        "- Links zählen und entfernen **IDEE:** Statt links zu entfernen diese mit einem eindeutigen Keyword ersetzen sowas wie <link>\n",
        "- Abkürzungen auflösen\n",
        "- Satzzeichen entfernen\n",
        "- Mehrere vorkommen von einem Buchstaben in einem Wort zusammenfassen auf zwei vorkomnisse (loooove -> loove)\n",
        "- Zahlen und Wörter mit Zahlen entfernen\n",
        "- MBTI Kennzeichungen entfernen, da diese in diesem Datensatz sehr häufig vertreten\n",
        "- Überflüssige Leerzeichen entfernt\n",
        "- Leere Listenelemente entfernt, da einige Posts nur aus Links bestanten, welche gelöscht wurden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ZUePLKqnAjYG"
      },
      "source": [
        "def dequote(s):\n",
        "    \"\"\"\n",
        "    If a string has single or double quotes around it, remove them.\n",
        "    Make sure the pair of quotes match.\n",
        "    If a matching pair of quotes is not found, return the string unchanged.\n",
        "    \"\"\"\n",
        "    if (s[0] == s[-1]) and s.startswith((\"'\", '\"')):\n",
        "        return s[1:-1]\n",
        "    return s\n",
        "\n",
        "#Anführungszeichen entfernen vorne und hinten\n",
        "df['posts_processed_dequote'] = df['posts'].apply(dequote)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRhd7AVsnZ9t"
      },
      "source": [
        "#Split an ||| \n",
        "df['posts_processed_dequote_split']=df['posts_processed_dequote'].apply(lambda x: x.split('|||'))\n",
        "print('||| Split')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqSEGIwTstGK"
      },
      "source": [
        "#lower\n",
        "df['posts_processed_dequote_split_lowercase']=df['posts_processed_dequote_split'].apply(lambda posts : list(map(str.lower, posts)))\n",
        "print('lowercase')\n",
        "print('Vergleich')\n",
        "print(df.at[0, 'posts_processed_dequote_split'])\n",
        "print(df.at[0, 'posts_processed_dequote_split_lowercase'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "X_oUn8lmAjYH"
      },
      "source": [
        "#Links in den Kommentaren Zählen\n",
        "\n",
        "def count_links_in_list (list):\n",
        "    link_counter = 0\n",
        "    for e in list:\n",
        "        link_counter += e.count('http')\n",
        "    return link_counter\n",
        "\n",
        "df['links_in_posts'] = df['posts_processed_dequote_split_lowercase'].apply(count_links_in_list)\n",
        "print('Count Links')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_Fe5XstwAjYH"
      },
      "source": [
        "#Durchschnitt Links pro 50 Posts\n",
        "df['avg_link_per_post'] = df['links_in_posts'].divide(50.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "g2JfwlPgAjYI"
      },
      "source": [
        "#Durchschnitt Links pro 50 Posts visualisieren\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.stripplot(x='type', y = 'avg_link_per_post', s = 4,data = df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OZy9TdveAjYI"
      },
      "source": [
        "# link removal\n",
        "def remove_URL(stringliteral):\n",
        "    \"\"\"Remove URLs from a sample string\"\"\"\n",
        "    return re.sub(r'https?://\\S+', '', str(stringliteral))\n",
        "\n",
        "df['posts_processed_dequote_split_lowercase_linksRemoved']=df['posts_processed_dequote_split_lowercase'].apply(lambda posts : list(map(remove_URL, posts)))\n",
        "print('Removed Links')\n",
        "print('Before')\n",
        "print(df.at[0, 'posts_processed_dequote_split_lowercase'])\n",
        "print('After')\n",
        "print(df.at[0, 'posts_processed_dequote_split_lowercase_linksRemoved'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "xheF7nNgAjYI"
      },
      "source": [
        "**Achtung** : Ab hier nicht mehr immer 50 Posts pro Zeile, da einige Posts nur aus einem Link bestanden und diese nun entfernt wurden -> Leeres Element in der Liste an Posts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "omeqQGyUAjYJ"
      },
      "source": [
        "# Dictionary of English Contractions\n",
        "contractions_dict = { \"ain't\": \"are not\",\"'s\":\" is\",\"aren't\": \"are not\",\n",
        "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\n",
        "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
        "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
        "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
        "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
        "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
        "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
        "                     \"i'd\": \"i would\", \"i'd've\": \"i would have\",\"i'll\": \"i will\",\n",
        "                     \"i'll've\": \"i will have\",\"i'm\": \"i am\",\"i've\": \"i have\", \"isn't\": \"is not\",\n",
        "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
        "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
        "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\",\n",
        "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
        "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
        "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
        "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
        "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
        "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
        "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
        "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
        "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
        "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
        "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
        "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
        "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
        "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
        "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
        "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
        "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
        "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
        "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
        "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
        "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
        "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
        "                     \"you've\": \"you have\"}\n",
        "\n",
        "\n",
        "\n",
        "# Regular expression for finding contractions\n",
        "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
        "\n",
        "# Function for expanding contractions\n",
        "def expand_contractions(text,contractions_dict=contractions_dict):\n",
        "  def replace(match):\n",
        "    return contractions_dict[match.group(0)]\n",
        "  return contractions_re.sub(replace, text)\n",
        "\n",
        "# Expanding Contractions\n",
        "#df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions']=df['posts_processed_dequote_split_lowercase_linksRemoved'].apply(lambda x : expand_contractions(x))\n",
        "df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions']=df['posts_processed_dequote_split_lowercase_linksRemoved'].apply(lambda posts : list(map(lambda post : expand_contractions(post), posts)))\n",
        "print('Abkürzungen entfernen')\n",
        "print('Before')\n",
        "print(df.at[1, 'posts_processed_dequote_split_lowercase_linksRemoved'])\n",
        "print('After')\n",
        "print(df.at[1, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FluLh54ZAjYJ"
      },
      "source": [
        "# Ausrufezeichen und Fragezeichen zählen, bevor sie entfernt werden\n",
        "\n",
        "def count_punctuation_in_list (list, punct):\n",
        "    counter = 0\n",
        "    for e in list:\n",
        "        counter += e.count(punct)\n",
        "    return counter\n",
        "\n",
        "df['exclamations_in_posts'] = df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions'].apply(lambda list : count_punctuation_in_list(list, '!'))\n",
        "df['questions_in_posts'] = df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions'].apply(lambda list : count_punctuation_in_list(list, '?'))\n",
        "print('Count Exclamation and Question Marks')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvtjITAUAjYK"
      },
      "source": [
        "# Benutzung von Exclamation und Question Marks visualisieren\n",
        "fig, ax1 = plt.subplots(figsize=(15,10))\n",
        "sns.stripplot(x='type', y = 'exclamations_in_posts', s = 4, color=\"#77b0ff\", alpha=0.6, data = df, ax=ax1)\n",
        "ax1.set_ylabel('Exclamation Marks', color=\"#77b0ff\")\n",
        "ax2 = ax1.twinx()\n",
        "sns.stripplot(x='type', y = 'questions_in_posts', s = 4, color=\"#ff7777\", alpha=0.3, data = df, ax=ax2)\n",
        "ax2.set_ylabel('Question Marks', color=\"#ff7777\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bh1Mjd76AjYK"
      },
      "source": [
        "#Satzzeichen mit Leerzeichen ersetzen (muss nach den contractions passieren, da diese ebenfalls entfernt werden )\n",
        "df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved']=df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions'].apply(lambda posts : list(map(lambda post: re.sub('[%s]' % re.escape(string.punctuation), ' ', post), posts)))\n",
        "print('Satzzeichen entfernt')\n",
        "print('Before')\n",
        "print(df.at[1, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions'])\n",
        "print('After')\n",
        "print(df.at[1, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9x5VcT-EAjYK"
      },
      "source": [
        "df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed']=df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved'].apply(lambda posts : list(map(lambda post : ''.join(''.join(s)[:2] for _, s in itertools.groupby(post)), posts)))\n",
        "print('Mehr als 2 Buchstaben hinereinander zusammengefasst')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_eg1nC3dSGF"
      },
      "source": [
        "#Zahlen und Wörter mit Zahlen entfernen\n",
        "df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers']=df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed'].apply(lambda posts : list(map(lambda post: re.sub('\\w*\\d\\w*','', post), posts)))\n",
        "print('Zahlen und Wörter mit Zahlen entfernt')\n",
        "print('Before')\n",
        "print(df.at[1, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed'])\n",
        "print('After')\n",
        "print(df.at[1, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xt-6JDKHAjYK"
      },
      "source": [
        "#MBTI Strings entfernen\n",
        "mbti_strings = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
        "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
        "mbti_strings = [e.lower() for e in mbti_strings]\n",
        "\n",
        "def remove_mbti_strings (post):\n",
        "    for e in mbti_strings:\n",
        "        post = post.replace(e,'')\n",
        "    return post\n",
        "\n",
        "df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings']=df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers'].apply(lambda posts : list(map(remove_mbti_strings, posts)))\n",
        "print('MBTI Strings entfernt')\n",
        "print('Before')\n",
        "print(df.at[3, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers'])\n",
        "print('After')\n",
        "print(df.at[3, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04G8TJ8pdTcX"
      },
      "source": [
        "# Mehrere Leerzeichen hintereinander mit einem Leerzeichen ersetzen.\n",
        "df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings_removedSpaces']=df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings'].apply(lambda posts : list(map(lambda post: re.sub(' +',' ',post), posts)))\n",
        "print('Leerzeichen zusammengefasst')\n",
        "print('Before')\n",
        "print(df.at[1, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings'])\n",
        "print('After')\n",
        "print(df.at[1, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings_removedSpaces'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ODA0lrCOAjYL"
      },
      "source": [
        "#Leere Listenelemente Löschen\n",
        "df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings_removedSpaces_removedEmptyPosts']=df['posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings_removedSpaces'].apply(lambda posts : list(filter(None, posts)))\n",
        "print('Leere Listenelemente gelöscht')\n",
        "print('Before')\n",
        "print(df.at[0, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings_removedSpaces'])\n",
        "print('After')\n",
        "print(df.at[0, 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings_removedSpaces_removedEmptyPosts'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zr8U725hAjYL"
      },
      "source": [
        "## Feature Extraktion\n",
        "**Achtung** : Ab hier hat die Liste an Posts eine unterschiedliche Anzahl an Elementen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "jyw44sV2AjYL"
      },
      "source": [
        "#Neuen Dataframe erstellen, mit dem Weitergearbeitet werden kann\n",
        "selected_columns = df[['type','exclamations_in_posts','questions_in_posts', 'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings_removedSpaces_removedEmptyPosts','links_in_posts','avg_link_per_post']]\n",
        "df_processed = selected_columns.copy()\n",
        "#Umbenennen\n",
        "df_processed = df_processed.rename(columns={'posts_processed_dequote_split_lowercase_linksRemoved_expandedContractions_punctuationRemoved_multipleLettersSquashed_removedNumbers_removedMbtiStrings_removedSpaces_removedEmptyPosts': 'posts'})\n",
        "df_processed['post_amount'] = df_processed['posts'].apply(lambda posts : len(posts))\n",
        "df_processed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "K_PgpDX0AjYM"
      },
      "source": [
        "#Wörter per Post\n",
        "df_processed['words_per_post'] = df_processed['posts'].apply(lambda posts : list(map(lambda post: len(post.split()), posts)))\n",
        "df_processed.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdHzlvpZdXRg",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "source": [
        "# Durchschnitt der Wörter pro Post\n",
        "df_processed['avg_words_of_posts'] = df_processed['words_per_post'].apply(lambda word_list : np.mean(word_list))\n",
        "print('Durchschnittl. words per Post')\n",
        "df_processed.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JUY_e_chAjYM"
      },
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "sns.stripplot(x = \"type\", y = \"avg_words_of_posts\", data=df_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Fuik60QgAjYM"
      },
      "source": [
        "#Buchstaben pro Posts\n",
        "df_processed['characters_per_post'] = df_processed['posts'].apply(lambda posts : list(map(lambda post: len(post) - post.count(' '), posts)))\n",
        "print('Buchstaben pro Post berechnen')\n",
        "print(df_processed.at[0,'posts'])\n",
        "df_processed.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GcRazF58AjYN"
      },
      "source": [
        "# Durchschnitt der Buchstaben pro Posts\n",
        "df_processed['avg_characters_of_posts'] = df_processed['characters_per_post'].apply(lambda word_list : np.mean(word_list))\n",
        "print('Durchschnittl. Buchstaben per Post')\n",
        "df_processed.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "jOIESldWAjYN"
      },
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "sns.stripplot(x = \"type\", y = \"avg_characters_of_posts\", data=df_processed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gurf6vMPAjYN"
      },
      "source": [
        "Wortwolken"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gImawq8KAjYN"
      },
      "source": [
        "# Wortwolken\n",
        "from functools import reduce\n",
        "import wordcloud as wc\n",
        "\n",
        "def splitAndCombine(l):\n",
        "  return reduce(lambda x,y:x+y,[x.split() for x in l])\n",
        "\n",
        "def countFrequency(l):\n",
        "  freq = {}\n",
        "  for word in l:\n",
        "    if (word in freq):\n",
        "      freq[word] += 1\n",
        "    else:\n",
        "      freq[word] = 1\n",
        "  \n",
        "  return freq\n",
        "\n",
        "typeWithPostsDict = {k:v for k,v in zip(df['type'], df['posts_processed_dequote_split'])}\n",
        "\n",
        "typeWithWordsDict = {t:splitAndCombine(pL) for t,pL in typeWithPostsDict.items()}\n",
        "\n",
        "typeWithWordFrequencyDict = {t:countFrequency(wL) for t,wL in typeWithWordsDict.items()}\n",
        "\n",
        "typeWithWordClouds = {t:wc.WordCloud(width=500, height=500, max_font_size=64).fit_words(wF) for t,wF in typeWithWordFrequencyDict.items()}\n",
        "\n",
        "for t,wC in typeWithWordClouds.items():\n",
        "  plt.figure(dpi=450)\n",
        "  plt.title(t)\n",
        "  plt.imshow(wC, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "  plt.savefig(str(t) + \".png\")\n",
        "  files.download(str(t) + \".png\") \n",
        "\n",
        "  plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT77cah6AjYN"
      },
      "source": [
        "Binäre repräsentation der MBTI-Typen erstellen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ydFCKk6PAjYN"
      },
      "source": [
        "#Aufteilung in mehrere binäre Entscheidungen als bool wert.\n",
        "map1 = {\"I\": 0, \"E\": 1}\n",
        "map2 = {\"N\": 0, \"S\": 1}\n",
        "map3 = {\"T\": 0, \"F\": 1}\n",
        "map4 = {\"J\": 0, \"P\": 1}\n",
        "df_processed['I-E'] = df_processed['type'].astype(str).str[0]\n",
        "df_processed['I-E'] = df_processed['I-E'].map(map1)\n",
        "df_processed['N-S'] = df_processed['type'].astype(str).str[1]\n",
        "df_processed['N-S'] = df_processed['N-S'].map(map2)\n",
        "df_processed['T-F'] = df_processed['type'].astype(str).str[2]\n",
        "df_processed['T-F'] = df_processed['T-F'].map(map3)\n",
        "df_processed['J-P'] = df_processed['type'].astype(str).str[3]\n",
        "df_processed['J-P'] = df_processed['J-P'].map(map4)\n",
        "df_processed.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GE082M2AjYO"
      },
      "source": [
        "Da in den Wortwolen so viele stop words sind wollte ich diese auch mal entfernen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "RThi23OoAjYO"
      },
      "source": [
        "Liste von einzelnen Posts zu einer einzigen Liste mit allen Wörtern aus den Posts zusammenfügen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "O01K6WvLAjYO"
      },
      "source": [
        "# Function to convert\n",
        "def listToString(s):\n",
        "\n",
        "    # initialize an empty string\n",
        "    str1 = \" \"\n",
        "\n",
        "    # return string\n",
        "    return (str1.join(s))\n",
        "\n",
        "df_processed['posts_as_one_list'] = df_processed['posts'].apply(lambda posts : listToString(posts))\n",
        "print('Posts zu einer Liste an Strings zusammenfügen')\n",
        "df_processed.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rirtFU0xAjYO"
      },
      "source": [
        "#Siehe https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "df_processed['posts_tokenized'] = df_processed['posts_as_one_list'].apply(word_tokenize)\n",
        "print('Tokenisieren')\n",
        "df_processed.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "aQ0rpZYGAjYO"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords (tokens):\n",
        "    result = []\n",
        "    for word in list(tokens):\n",
        "        if word not in stop_words:\n",
        "            result.append(word)\n",
        "    return result\n",
        "\n",
        "\n",
        "df_processed['posts_without_stopwords_tokenized'] = df_processed['posts_tokenized'].apply(remove_stopwords)\n",
        "print('Stop wörter entfernen')\n",
        "df_processed.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "sVsMWQZRAjYO"
      },
      "source": [
        "Wortwolken nachdem Stopwörter entfernt wurden"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HLAEumaUAjYO"
      },
      "source": [
        "def splitAndCombine(l):\n",
        "  return reduce(lambda x,y:x+y,[x.split() for x in l])\n",
        "\n",
        "def countFrequency(l):\n",
        "  freq = {}\n",
        "  for word in l:\n",
        "    if (word in freq):\n",
        "      freq[word] += 1\n",
        "    else:\n",
        "      freq[word] = 1\n",
        "\n",
        "  return freq\n",
        "\n",
        "typeWithPostsDict = {k:v for k,v in zip(df_processed['type'], df_processed['posts_without_stopwords_tokenized'])}\n",
        "\n",
        "typeWithWordsDict = {t:splitAndCombine(pL) for t,pL in typeWithPostsDict.items()}\n",
        "\n",
        "typeWithWordFrequencyDict = {t:countFrequency(wL) for t,wL in typeWithWordsDict.items()}\n",
        "\n",
        "typeWithWordClouds = {t:wc.WordCloud(width=500, height=500, max_font_size=64).fit_words(wF) for t,wF in typeWithWordFrequencyDict.items()}\n",
        "\n",
        "for t,wC in typeWithWordClouds.items():\n",
        "  plt.figure(dpi=150)\n",
        "  plt.title(t)\n",
        "  plt.imshow(wC, interpolation='bilinear')\n",
        "  plt.axis(\"off\")\n",
        "\n",
        "  if str(t) == \"ENTP\" :\n",
        "    plt.savefig(str(t) + \".png\")\n",
        "    files.download(str(t) + \".png\")\n",
        "\n",
        "  plt.show"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_Qfint8QAjYO"
      },
      "source": [
        "**Da die Anzahl an Posts unterschiedlich sind müssen wir mit Durchnschnitten rechnen, daher müssen die ? und ! noch als durchschnitt berechnet werden**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "WReMh9iyAjYO"
      },
      "source": [
        "df_processed['avg_exclamations_in_posts'] = np.divide(df_processed['exclamations_in_posts'],df_processed['post_amount'])\n",
        "df_processed['avg_questions_in_posts'] = np.divide(df_processed['questions_in_posts'],df_processed['post_amount'])\n",
        "df_processed.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdMXphhHAjYO"
      },
      "source": [
        "value_list = [1]\n",
        "df_processed['I_E'] = df_processed['I-E']\n",
        "df_processed['N_S'] = df_processed['N-S']\n",
        "df_processed['T_F'] = df_processed['T-F']\n",
        "df_processed['J_P'] = df_processed['J-P']\n",
        "#df_processed[df_processed.I_E.isin(value_list)]\n",
        "df_IE = df_processed.groupby('I_E')['links_in_posts', 'exclamations_in_posts', 'questions_in_posts', 'avg_words_of_posts', 'post_amount'].agg('sum')\n",
        "df_NS = df_processed.groupby('N_S')['links_in_posts', 'exclamations_in_posts', 'questions_in_posts', 'avg_words_of_posts', 'post_amount'].agg('sum')\n",
        "df_TF = df_processed.groupby('T_F')['links_in_posts', 'exclamations_in_posts', 'questions_in_posts', 'avg_words_of_posts', 'post_amount'].agg('sum')\n",
        "df_JP = df_processed.groupby('J_P')['links_in_posts', 'exclamations_in_posts', 'questions_in_posts', 'avg_words_of_posts', 'post_amount'].agg('sum')\n",
        "\n",
        "df_IE['links_in_posts'] = np.divide(df_IE['links_in_posts'],df_IE['post_amount'])\n",
        "df_IE['exclamations_in_posts'] = np.divide(df_IE['exclamations_in_posts'],df_IE['post_amount'])\n",
        "df_IE['questions_in_posts'] = np.divide(df_IE['questions_in_posts'],df_IE['post_amount'])\n",
        "df_IE['avg_words_of_posts'] = np.divide(df_IE['avg_words_of_posts'],df_IE['post_amount'])\n",
        "\n",
        "df_NS['links_in_posts'] = np.divide(df_NS['links_in_posts'],df_NS['post_amount'])\n",
        "df_NS['exclamations_in_posts'] = np.divide(df_NS['exclamations_in_posts'],df_NS['post_amount'])\n",
        "df_NS['questions_in_posts'] = np.divide(df_NS['questions_in_posts'],df_NS['post_amount'])\n",
        "df_NS['avg_words_of_posts'] = np.divide(df_NS['avg_words_of_posts'],df_NS['post_amount'])\n",
        "\n",
        "df_TF['links_in_posts'] = np.divide(df_TF['links_in_posts'],df_TF['post_amount'])\n",
        "df_TF['exclamations_in_posts'] = np.divide(df_TF['exclamations_in_posts'],df_TF['post_amount'])\n",
        "df_TF['questions_in_posts'] = np.divide(df_TF['questions_in_posts'],df_TF['post_amount'])\n",
        "df_TF['avg_words_of_posts'] = np.divide(df_TF['avg_words_of_posts'],df_TF['post_amount'])\n",
        "\n",
        "df_JP['links_in_posts'] = np.divide(df_JP['links_in_posts'],df_JP['post_amount'])\n",
        "df_JP['exclamations_in_posts'] = np.divide(df_JP['exclamations_in_posts'],df_JP['post_amount'])\n",
        "df_JP['questions_in_posts'] = np.divide(df_JP['questions_in_posts'],df_JP['post_amount'])\n",
        "df_JP['avg_words_of_posts'] = np.divide(df_JP['avg_words_of_posts'],df_JP['post_amount'])\n",
        "df_IE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egw_sgICAjYO"
      },
      "source": [
        "df_NS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lpperOUAjYP"
      },
      "source": [
        "df_TF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5PcvLrqAjYQ"
      },
      "source": [
        "df_JP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epK026jxAjYQ"
      },
      "source": [
        "fig, ax1 = plt.subplots(figsize=(8,6))\n",
        "sns.stripplot(x=df_IE.index, y = 'links_in_posts', s = 4, color=\"#77b0ff\", data = df_IE, ax=ax1)\n",
        "ax1.set_ylabel('Average Exclamation Marks', color=\"#77b0ff\")\n",
        "ax2 = ax1.twinx()\n",
        "sns.stripplot(x=df_IE.index, y = 'exclamations_in_posts', s = 4, color=\"#ff7777\", data = df_IE, ax=ax2)\n",
        "ax2.set_ylabel('Average Question Marks', color=\"#ff7777\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Petywy14AjYQ"
      },
      "source": [
        "# Durchschnitt von Exclamation und Question Marks visualisieren\n",
        "fig, ax1 = plt.subplots(figsize=(15,10))\n",
        "sns.stripplot(x='type', y = 'avg_exclamations_in_posts', s = 4, color=\"#77b0ff\", alpha=0.6, data = df_processed, ax=ax1)\n",
        "ax1.set_ylabel('Average Exclamation Marks', color=\"#77b0ff\")\n",
        "ax2 = ax1.twinx()\n",
        "#sns.stripplot(x='type', y = 'avg_questions_in_posts', s = 4, color=\"#ff7777\", alpha=0.3, data = df, ax=ax2)\n",
        "ax2.set_ylabel('Average Question Marks', color=\"#ff7777\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "YDeJp1QZAjYQ"
      },
      "source": [
        "Hier der Dataframe mit den ganzen Features, welche wir für die verschiedene ML ansätze benutzen können (**Ggf. ergänzen**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9DofxjQAjYR"
      },
      "source": [
        "# Durchschnitt von Exclamation pro Ausprägung\n",
        "fig, ax1 = plt.subplots(figsize=(15,10))\n",
        "sns.stripplot(x='I_E', y = 'avg_exclamations_in_posts', s = 4, color=\"#77b0ff\", alpha=0.6, data = df_processed[df_processed.I_E.isin(value_list)], ax=ax1)\n",
        "ax1.set_ylabel('Average Exclamation Marks', color=\"#77b0ff\")\n",
        "ax2 = ax1.twinx()\n",
        "sns.stripplot(x='I_E', y = 'avg_questions_in_posts', s = 4, color=\"#ff7777\", alpha=0.3, data = df_processed[~df_processed.I_E.isin(value_list)], ax=ax2)\n",
        "ax2.set_ylabel('Average Question Marks', color=\"#ff7777\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "7mroioq-AjYR"
      },
      "source": [
        "#Neuen Dataframe erstellen, mit dem Weitergearbeitet werden kann\n",
        "selected_columns_2 = df_processed[['type','I-E','N-S','T-F','J-P','posts_without_stopwords_tokenized','avg_exclamations_in_posts', 'avg_questions_in_posts','avg_words_of_posts','avg_link_per_post']]\n",
        "df_final = selected_columns_2.copy()\n",
        "df_final.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ShkhkzCbAjYR"
      },
      "source": [
        "import tensorflow\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "num_words = 10000\n",
        "oov_token = '-NA-'\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(df_final['posts_without_stopwords_tokenized'])\n",
        "df_final['tokens_keras'] = tokenizer.texts_to_sequences(df_final['posts_without_stopwords_tokenized'])\n",
        "\n",
        "# summarize what was learned\n",
        "#print(tokenizer.word_counts)  #A dictionary of words and their counts.\n",
        "#print(tokenizer.document_count) #An integer count of the total number of documents that were used to fit the Tokenizer.\n",
        "print('Zuordnung')\n",
        "print(tokenizer.word_index) #A dictionary of words and their uniquely assigned integers.\n",
        "#print(tokenizer.word_docs) # A dictionary of words and how many documents each appeared in.\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BsZXvw_AjYR"
      },
      "source": [
        "Hier kommt der Split der Daten und anschließend das Training, aber vorher müssen diese noch normalisiert werden und richtig encoded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjWx4ylOAjYR"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def one_hot_encode_sequence(seq, dim):\n",
        "    result = np.zeros((len(seq), dim)) #create a all-zero matrix of shape [seq, dim]\n",
        "    for row, col in enumerate(seq):\n",
        "        result[row, col] = 1\n",
        "    return result\n",
        "\n",
        "# input (X) and output (y) columns\n",
        "x_raw = df_final.drop(['type'], axis=1).values\n",
        "\n",
        "data = np.array([x[9] for x in x_raw])\n",
        "labels = np.array([list(x) for x in df_final['type'].values])\n",
        "\n",
        "print(len(data))\n",
        "print(len(labels))\n",
        "\n",
        "print(labels)\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(labels)\n",
        "\n",
        "print(labels)\n",
        "\n",
        "for (i, label) in enumerate(mlb.classes_):\n",
        "\tprint(\"{}. {}\".format(i + 1, label))\n",
        "\n",
        "(train_data, test_data, train_labels, test_labels) = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Train Data with {} samples. Example: {} has {} words\".format(len(train_data), train_data[0], len(train_data[0])))\n",
        "print(\"Train Data {} samples.  Label Example: {}\".format(len(train_labels),train_labels[0]))\n",
        "print(\"Test Data has {} samples\".format(test_data.shape[0]))\n",
        "\n",
        "x_train = one_hot_encode_sequence(train_data, num_words)\n",
        "x_test = one_hot_encode_sequence(test_data, num_words)\n",
        "\n",
        "print(x_train[1,:100])\n",
        "print(\"x_train is a matrix of {}x{}\".format(x_train.shape[0],x_train.shape[1]))\n",
        "print(\"x_test is a matrix of {}x{}\".format(x_test.shape[0],x_test.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHjww8HAjYS"
      },
      "source": [
        "#put some - e.g. 20% - train samples as cross validation (dev) set aside\n",
        "x_train_samples, x_val_samples, y_train_samples, y_val_samples = train_test_split(x_train, train_labels, train_size=0.80)\n",
        "\n",
        "print(\"x_train_samples is a matrix of {}x{}\".format(x_train_samples.shape[0],x_train_samples.shape[1]))\n",
        "print(\"y_train_samples is an array of length:{}\".format(y_train_samples.shape[0]))\n",
        "print(\"x_val_samples is a matrix of {}x{}\".format(x_val_samples.shape[0],x_val_samples.shape[1]))\n",
        "print(\"y_val_samples is an array of length:{}\".format(y_val_samples.shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBtgmtEjAjYS"
      },
      "source": [
        "from keras import models\n",
        "from keras import utils\n",
        "from keras import layers\n",
        "from keras import losses\n",
        "from keras import optimizers\n",
        "\n",
        "#hyperparameter\n",
        "\n",
        "hiddenlayer_neurons=64\n",
        "hiddenlayer_activation_function='relu'\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "#1st layer - input layer\n",
        "model.add(layers.Dense(units= hiddenlayer_neurons, activation=hiddenlayer_activation_function, input_shape=(num_words,)))\n",
        "#2nd layer\n",
        "model.add(layers.Dense(units= hiddenlayer_neurons, activation=hiddenlayer_activation_function))\n",
        "\n",
        "#...\n",
        "model.add(layers.Dense(units= hiddenlayer_neurons, activation=hiddenlayer_activation_function))\n",
        "model.add(layers.Dense(units= hiddenlayer_neurons, activation=hiddenlayer_activation_function))\n",
        "model.add(layers.Dense(units= hiddenlayer_neurons, activation=hiddenlayer_activation_function))\n",
        "model.add(layers.Dense(units= hiddenlayer_neurons, activation=hiddenlayer_activation_function))\n",
        "model.add(layers.Dense(units= hiddenlayer_neurons, activation=hiddenlayer_activation_function))\n",
        "model.add(layers.Dense(units= hiddenlayer_neurons, activation=hiddenlayer_activation_function))\n",
        "model.add(layers.Dense(units= hiddenlayer_neurons, activation=hiddenlayer_activation_function))\n",
        "\n",
        "\n",
        "#last layer -output layer - has 46 outputs (len of topics array)\n",
        "model.add(layers.Dense(len(mlb.classes_), activation='sigmoid'))\n",
        "\n",
        "\n",
        "#An optimizer. The gradient descent algorithm - this could be the string identifier of an existing optimizer\n",
        "# such as rmsprop or adagrad\n",
        "# see also: https://keras.io/optimizers/\n",
        "\n",
        "#loss: The loss function - the objective that the model will try to minimize. \n",
        "#It can be the string identifierof an existing loss function (such as categorical_crossentropy or mse)\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.0001),\n",
        "             loss = losses.binary_crossentropy, #.mean_squared_error, #categorical_crossentropy\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGqqv4wpAjYS"
      },
      "source": [
        "history = model.fit (\n",
        "                    x_train_samples,\n",
        "                    y_train_samples,\n",
        "                    epochs = 30, # how many time to repeat the learning process for the entire dataset\n",
        "                    batch_size=128, # we will reset the weights batch-size wise (mini batch GD)\n",
        "                    validation_data = (x_val_samples, y_val_samples)\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urKqVU2oAjYS"
      },
      "source": [
        "result = model.evaluate(x_test,test_labels) # final evaluation of the model\n",
        "print(\"result on the testset: accuracy={}\".format(result[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-QHpkLVAjYS"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "history_dict = history.history\n",
        "#print(history_dict)\n",
        "\n",
        "amount_epochs = len(history_dict['accuracy']) \n",
        "plt.plot(range(1, amount_epochs+1), history_dict['loss'], 'ro', label='Training Set Loss')\n",
        "plt.plot(range(1, amount_epochs+1), history_dict['val_loss'], 'r', label='Validation Set Loss')\n",
        "\n",
        "plt.xlabel('Training Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "plt.plot(range(1, amount_epochs+1), history_dict['accuracy'], 'bo', label='Training Set Accuracy')\n",
        "plt.plot(range(1, amount_epochs+1), history_dict['val_accuracy'], 'b', label='Validation Set Accuracy')\n",
        "\n",
        "plt.xlabel('Training Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyjMMvXWAjYT"
      },
      "source": [
        "predictions = model.predict(x_test)\n",
        "print(\"predictions is a matrix of {}x{}\".format(predictions.shape[0],predictions.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9gMVfvfAjYT"
      },
      "source": [
        "index = 1734\n",
        "print(\"prediction: \\n {} \\n\\n\"\n",
        "      .format(predictions[index]\n",
        "      ))\n",
        "\n",
        "print(\"ground truth label class: {}\".format(test_labels[index]))\n",
        "\n",
        "result = []\n",
        "\n",
        "im_classlist =  [x for x in predictions[index]]\n",
        "m_classList = [x for x in predictions[index]]\n",
        "\n",
        "for i in range(4) :\n",
        "  print(m_classList)\n",
        "  maxItem = max(m_classList)\n",
        "  result.append(im_classlist.index(maxItem))\n",
        "  m_classList.remove(maxItem)\n",
        "\n",
        "print(\"found indices: \" + str(result))\n",
        "\n",
        "print(\"class in binarizer: \" + str(mlb.classes_))\n",
        "\n",
        "# I E S N F T J P\n",
        "letters = [0, 2, 6, 4, 1, 7, 3, 5]\n",
        "\n",
        "result.sort(key = lambda i: letters.index(i))\n",
        "\n",
        "print(\"MBTI: {}\".format([mlb.classes_[x] for x in result]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFnvfPuPX6gi"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "sentences = df_processed['posts_without_stopwords_tokenized'].apply(lambda x : ' '.join(x))\n",
        "\n",
        "print(sentences)\n",
        "\n",
        "tfid = TfidfVectorizer(ngram_range=(2,4), min_df=0.005, sublinear_tf=False)\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "labels = np.array([list(x) for x in df_processed['type'].values])\n",
        "\n",
        "mlb = MultiLabelBinarizer()\n",
        "labels = mlb.fit_transform(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB8--dukkZyq"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "(train_data, test_data, train_labels, test_labels) = train_test_split(tfid.fit_transform(df_final['sentences'].reset_index(drop=True)).sorted_indices(), labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "\n",
        "x_train_samples, x_val_samples, y_train_samples, y_val_samples = train_test_split(train_data, train_labels, train_size=0.80)\n",
        "\n",
        "print(x_train_samples.shape)\n",
        "print(x_val_samples.shape)\n",
        "\n",
        "print(x_train_samples[0])\n",
        "print(y_train_samples[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYHYAn-M6QS1"
      },
      "source": [
        "print(x_train_samples[1])\n",
        "print(y_train_samples[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrNYIhtBlnrE"
      },
      "source": [
        "model_tdif = models.Sequential()\n",
        "model_tdif.add(layers.Dense(units=64 , activation='relu', input_shape=(7082,)))\n",
        "model_tdif.add(layers.Dense(units=64 , activation='relu'))\n",
        "model_tdif.add(layers.Dense(len(mlb.classes_), activation='sigmoid'))\n",
        "\n",
        "model_tdif.compile(optimizer=optimizers.RMSprop(learning_rate=0.00001),\n",
        "             loss = losses.categorical_crossentropy,\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXEYcDn5l3Ge"
      },
      "source": [
        "history = model_tdif.fit (\n",
        "                    x_train_samples,\n",
        "                    y_train_samples,\n",
        "                    epochs = 30,\n",
        "                    batch_size=128,\n",
        "                    verbose=1,\n",
        "                    validation_data = (x_val_samples, y_val_samples)\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8HVUYYul_R_"
      },
      "source": [
        "result = model_tdif.evaluate(test_data,test_labels) # final evaluation of the model\n",
        "print(\"result on the testset: accuracy={}\".format(result[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oQWgzQRlEoG"
      },
      "source": [
        "predictions = model_tdif.predict(test_data)\n",
        "print(\"predictions is a matrix of {}x{}\".format(predictions.shape[0],predictions.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiFF01_Vk6rW"
      },
      "source": [
        "index = 1734\n",
        "print(\"prediction: \\n {} \\n\\n\"\n",
        "      .format(predictions[index]\n",
        "      ))\n",
        "\n",
        "print(\"ground truth label class: {}\".format(test_labels[index]))\n",
        "\n",
        "result = []\n",
        "\n",
        "im_classlist =  [x for x in predictions[index]]\n",
        "m_classList = [x for x in predictions[index]]\n",
        "\n",
        "for i in range(4) :\n",
        "  print(m_classList)\n",
        "  maxItem = max(m_classList)\n",
        "  result.append(im_classlist.index(maxItem))\n",
        "  m_classList.remove(maxItem)\n",
        "\n",
        "print(\"found indices: \" + str(result))\n",
        "\n",
        "print(\"class in binarizer: \" + str(mlb.classes_))\n",
        "\n",
        "# I E S N F T J P\n",
        "letters = [0, 2, 6, 4, 1, 7, 3, 5]\n",
        "\n",
        "result.sort(key = lambda i: letters.index(i))\n",
        "\n",
        "print(\"MBTI: {}\".format([mlb.classes_[x] for x in result]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZ_vakmVfXwe"
      },
      "source": [
        "# **Multi-Class Text Classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53JqXr1Qn1_g"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "types = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n",
        "       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n",
        "\n",
        "sentences = df_processed['posts_without_stopwords_tokenized'].apply(lambda x : ' '.join(x))\n",
        "\n",
        "print(sentences)\n",
        "print(df_processed['type'])\n",
        "\n",
        "X = sentences\n",
        "y = df_processed['type']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfcDlN8day7k"
      },
      "source": [
        "nb = Pipeline([('vect', CountVectorizer()),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=types))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQOl-75dn7k4"
      },
      "source": [
        "nb_gram = Pipeline([('vect', CountVectorizer(ngram_range=(2,3))),\n",
        "               ('tfidf', TfidfTransformer()),\n",
        "               ('clf', MultinomialNB()),\n",
        "              ])\n",
        "\n",
        "nb_gram.fit((X_train, y_train)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_pred = nb_gram.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=types))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzqS8WncazBz"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "               ])\n",
        "\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "y_pred = sgd.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=types))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEVY9kzukEPq"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_gram = Pipeline([('vect', CountVectorizer(ngram_range=(2,3))),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
        "               ])\n",
        "\n",
        "sgd_gram.fit(X_train, y_train)\n",
        "\n",
        "y_pred = sgd_gram.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=types))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYkMqk6VazNh"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, max_iter=150)),\n",
        "               ])\n",
        "\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=types))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0InnaCeukkg"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg_gram = Pipeline([('vect', CountVectorizer(ngram_range=(2,3))),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, max_iter=100)),\n",
        "               ])\n",
        "\n",
        "logreg_gram.fit(X_train, y_train)\n",
        "\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=types))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UK5dnyVofHKX"
      },
      "source": [
        "https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHk08WS_tkQ5"
      },
      "source": [
        "# **Multi-Label Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gy9wfuWf8OB4"
      },
      "source": [
        "Run **ONE** of the vectorizers based on preference:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4XEA95UaEbG"
      },
      "source": [
        "feature_count = 50000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVBA3Lcz8WWX"
      },
      "source": [
        "***NGram (1, 1)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a04J4YpZqu9b"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1,1), lowercase=False,max_features=feature_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM0TMVhoSWHE"
      },
      "source": [
        "***NGram (1, 2)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMbTPJr7SU3x"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=False ,max_features=feature_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Se3TFf6Sa7N"
      },
      "source": [
        "***NGram (2, 2)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkgFes58SaLh"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(2,2), lowercase=False ,max_features=feature_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVTefBXS8Z6j"
      },
      "source": [
        "**NGram (1, 3)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nq0JtUl8EL1"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1,3), lowercase=False, max_features=feature_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6nDwBB8ahG"
      },
      "source": [
        "**NGram (2, 3)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtVBcige8EY-"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(2,3), lowercase=False, max_features=feature_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgiPlgg7K_0b"
      },
      "source": [
        "**NGram (3, 3)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfF0NL9JK_I_"
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(3,3), lowercase=False, max_features=feature_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjbaMa12UmIH"
      },
      "source": [
        "**Binary Classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_02TD6Eq6Bd"
      },
      "source": [
        "df_split_types = df_processed['type'].apply(lambda x : [c for c in x])\n",
        "\n",
        "flippedClasses =    ['E','S','T','J']\n",
        "unflippedClasses =  ['I','N','F','P']\n",
        "\n",
        "df_binary_classes = df_split_types.apply(lambda x : [1 if c in flippedClasses else 0 for c in x])\n",
        "\n",
        "print(df_binary_classes.values)\n",
        "\n",
        "binary_class_arr =  np.asarray(df_binary_classes.values.tolist(), np.int64)\n",
        "\n",
        "print(binary_class_arr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G40nkefa-S73"
      },
      "source": [
        "Fitting and Transforming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZQCIBzm-Msm"
      },
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "y = binary_class_arr\n",
        "\n",
        "vectorizer.fit(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "X_train_tfidf = vectorizer.transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-n9_HEt-dJ7c"
      },
      "source": [
        "len(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YMPvLQ0C7fs"
      },
      "source": [
        "row = X_train_tfidf.getrow(0)\n",
        "feature_count = len(vectorizer.vocabulary_)\n",
        "\n",
        "sum = 0.0\n",
        "\n",
        "for y in range(size):\n",
        "  row = X_train_tfidf.getrow(y)\n",
        "  row_sum = 0.0\n",
        "  for x in range(feature_count):\n",
        "    row_sum += row[0,x]\n",
        "  row_sum /= feature_count\n",
        "  sum += row_sum\n",
        "  print(sum)\n",
        "\n",
        "print(sum / size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R6lSWRE-5NY"
      },
      "source": [
        "def value_sum (values):\n",
        "  sum = 0.0\n",
        "  for value in values:\n",
        "    sum += value\n",
        "  return sum\n",
        "\n",
        "vocab_dict = vectorizer.vocabulary_\n",
        "\n",
        "length = len(vocab_dict)\n",
        "sum = value_sum(vectorizer.vocabulary_.values())\n",
        "\n",
        "print(sum / len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6wyfRrY817s"
      },
      "source": [
        "Visualation of Frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FApqIx1s81Lr"
      },
      "source": [
        "from yellowbrick.text import FreqDistVisualizer\n",
        "features = vectorizer.get_feature_names()\n",
        "visualizer = FreqDistVisualizer(features=features, orient='v', n=75, size=(1080, 540))\n",
        "visualizer.fit(X_train_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJXVZZrYz0FM"
      },
      "source": [
        "features = vectorizer.get_feature_names()\n",
        "\n",
        "td_idf_m0 = X_train_tfidf.getrow(0)\n",
        "\n",
        "size = td_idf_m0.shape[1]\n",
        "\n",
        "td_idf_arr0 = []\n",
        "\n",
        "td_idf_arr0 = [td_idf_m0[0,i] for i in range(size)]\n",
        "\n",
        "td_idf_dict0 = {features[i]: td_idf_arr0[i] for i in range(size) if td_idf_arr0[i] >= 0.02}\n",
        "\n",
        "plt.figure(dpi=300)\n",
        "\n",
        "plt.bar(*zip(*td_idf_dict0.items()))\n",
        "plt.xticks(rotation=90)\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.tick_params(axis = 'both', which = 'major', labelsize = 12)\n",
        "ax.tick_params(axis = 'both', which = 'minor', labelsize = 8)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "#plt.savefig(\"td_idf_0.png\")\n",
        "#files.download(\"td_idf_0.png\") \n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wENS4vn1-4uR"
      },
      "source": [
        "**Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AYaisjlDA5p"
      },
      "source": [
        "https://www.geeksforgeeks.org/an-introduction-to-multilabel-classification/\n",
        "\n",
        "https://medium.com/technovators/machine-learning-based-multi-label-text-classification-9a0e17f88bb4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOBJhhu1DMS-"
      },
      "source": [
        "!pip install scikit-multilearn\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from skmultilearn.adapt import MLkNN\n",
        "from sklearn.metrics import hamming_loss, accuracy_score, f1_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from skmultilearn.problem_transform import BinaryRelevance\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "from sklearn.neural_network import MLPClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bh1SgeBCOvY"
      },
      "source": [
        "NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHby0Cb-Bi0N"
      },
      "source": [
        "nbClassifier = OneVsRestClassifier(MultinomialNB())\n",
        "nbClassifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "nbPreds = nbClassifier.predict(X_test_tfidf)\n",
        "\n",
        "print(accuracy_score(y_test, nbPreds))\n",
        "print(hamming_loss(y_test, nbPreds))\n",
        "print(f1_score(y_test, nbPreds, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k18AHPXd8k3S"
      },
      "source": [
        "Multi-Label K-Nearest-Neighbour"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNJe5f59rNTh"
      },
      "source": [
        "mlknn_classifier = MLkNN()\n",
        "mlknn_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "mlknnPreds = mlknn_classifier.predict(X_test_tfidf)\n",
        "  \n",
        "print(accuracy_score(y_test, mlknnPreds))\n",
        "print(hamming_loss(y_test, mlknnPreds))\n",
        "print(f1_score(y_test, mlknnPreds, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxaFHYcZAbxr"
      },
      "source": [
        "Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjgyZiFdAaza"
      },
      "source": [
        "svmClassifier = OneVsRestClassifier(LinearSVC(), n_jobs=-1)\n",
        "svmClassifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "svmPreds = svmClassifier.predict(X_test_tfidf)\n",
        "\n",
        "print(accuracy_score(y_test, svmPreds))\n",
        "print(hamming_loss(y_test, svmPreds))\n",
        "print(f1_score(y_test, svmPreds, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LorDEaLKCTVR"
      },
      "source": [
        "Binary Relevance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSYgZQLiCSUL"
      },
      "source": [
        "binaryRelSVC = BinaryRelevance(LinearSVC())\n",
        "binaryRelSVC.fit(X_train_tfidf, y_train)\n",
        "\n",
        "binaryRelSVCPreds = binaryRelSVC.predict(X_test_tfidf)\n",
        "\n",
        "print(accuracy_score(y_test, binaryRelSVCPreds))\n",
        "print(hamming_loss(y_test, binaryRelSVCPreds))\n",
        "print(f1_score(y_test, binaryRelSVCPreds, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NdUGrD__21Z"
      },
      "source": [
        "Label Powerset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQc4QL7Q6hH2"
      },
      "source": [
        "powerSetSVC = LabelPowerset(LinearSVC())\n",
        "powerSetSVC.fit(X_train_tfidf, y_train)\n",
        "\n",
        "powerSetSVCPreds = powerSetSVC.predict(X_test_tfidf)\n",
        "\n",
        "print(accuracy_score(y_test, powerSetSVCPreds))\n",
        "print(hamming_loss(y_test, powerSetSVCPreds))\n",
        "print(f1_score(y_test, powerSetSVCPreds, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSqmdfakMEv8"
      },
      "source": [
        "Multi-layer Perceptron classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDRw8aVoL8La"
      },
      "source": [
        "mlp = MLPClassifier(random_state=42, max_iter=50, hidden_layer_sizes=64, verbose=True)\n",
        "mlp.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(mlp.score(X_test_tfidf, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzWS1m97kIGo"
      },
      "source": [
        "print(mlp.loss_curve_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbi-Cw-dVdh7"
      },
      "source": [
        "Visualization of specfic prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phifzCAVQndT"
      },
      "source": [
        "index = 254\n",
        "\n",
        "def binary_to_mbti_str(binary_arr):\n",
        "  return \"\".join([flippedClasses[i] if x == 1 else unflippedClasses[i] for i, x in enumerate(binary_arr)])\n",
        "\n",
        "print(\"prediction: \\n {} \\n\"\n",
        "      .format(powerSetSVCPreds[index]\n",
        "      ))\n",
        "\n",
        "print(\"ground truth label class:\\t{}\".format(y_test[index]))\n",
        "print(\"ground truth MBTI: \\t\\t{}\".format(binary_to_mbti_str(y_test[index])))\n",
        "\n",
        "setClasses = powerSetSVCPreds.rows[index]\n",
        "\n",
        "result = [0] * 4\n",
        "\n",
        "for c in setClasses:\n",
        "  result[c] = 1\n",
        "\n",
        "print(\"\\nfound classes:\\t\\t\\t{}\".format(np.array(result)))\n",
        "print(\"found MBTI: \\t\\t\\t{}\".format(binary_to_mbti_str(np.array(result))))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}